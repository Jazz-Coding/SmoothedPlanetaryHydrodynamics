//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31833905
// Cuda compilation tools, release 11.8, V11.8.89
// Based on NVVM 7.0.1
//

.version 7.8
.target sm_52
.address_size 64

	// .globl	calculate_momentum
.extern .const .align 4 .u32 N_particles;
.extern .const .align 4 .f32 K;
.extern .const .align 4 .f32 adiabaticIndex;

.visible .entry calculate_momentum(
	.param .u64 calculate_momentum_param_0,
	.param .u64 calculate_momentum_param_1
)
{
	.reg .pred 	%p<29>;
	.reg .f32 	%f<113>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [calculate_momentum_param_0];
	ld.param.u64 	%rd4, [calculate_momentum_param_1];
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r1, %r2, %r3, %r4;
	ld.const.u32 	%r5, [N_particles];
	setp.ge.s32 	%p2, %r1, %r5;
	@%p2 bra 	$L__BB0_16;

	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd1, %rd3;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.const.f32 	%f2, [adiabaticIndex];
	mul.f32 	%f19, %f2, 0f3F000000;
	cvt.rzi.f32.f32 	%f20, %f19;
	add.f32 	%f21, %f20, %f20;
	sub.f32 	%f22, %f2, %f21;
	abs.f32 	%f3, %f22;
	ld.global.f32 	%f4, [%rd7];
	abs.f32 	%f5, %f4;
	setp.lt.f32 	%p3, %f5, 0f00800000;
	mul.f32 	%f23, %f5, 0f4B800000;
	selp.f32 	%f24, %f23, %f5, %p3;
	selp.f32 	%f25, 0fC3170000, 0fC2FE0000, %p3;
	mov.b32 	%r6, %f24;
	and.b32  	%r7, %r6, 8388607;
	or.b32  	%r8, %r7, 1065353216;
	mov.b32 	%f26, %r8;
	shr.u32 	%r9, %r6, 23;
	cvt.rn.f32.u32 	%f27, %r9;
	add.f32 	%f28, %f25, %f27;
	setp.gt.f32 	%p4, %f26, 0f3FB504F3;
	mul.f32 	%f29, %f26, 0f3F000000;
	add.f32 	%f30, %f28, 0f3F800000;
	selp.f32 	%f31, %f30, %f28, %p4;
	selp.f32 	%f32, %f29, %f26, %p4;
	add.f32 	%f33, %f32, 0fBF800000;
	add.f32 	%f34, %f32, 0f3F800000;
	rcp.approx.ftz.f32 	%f35, %f34;
	add.f32 	%f36, %f33, %f33;
	mul.f32 	%f37, %f36, %f35;
	mul.f32 	%f38, %f37, %f37;
	mov.f32 	%f39, 0f3C4CAF63;
	mov.f32 	%f40, 0f3B18F0FE;
	fma.rn.f32 	%f41, %f40, %f38, %f39;
	mov.f32 	%f42, 0f3DAAAABD;
	fma.rn.f32 	%f43, %f41, %f38, %f42;
	mul.rn.f32 	%f44, %f43, %f38;
	mul.rn.f32 	%f45, %f44, %f37;
	sub.f32 	%f46, %f33, %f37;
	add.f32 	%f47, %f46, %f46;
	neg.f32 	%f48, %f37;
	fma.rn.f32 	%f49, %f48, %f33, %f47;
	mul.rn.f32 	%f50, %f35, %f49;
	add.f32 	%f51, %f45, %f37;
	sub.f32 	%f52, %f37, %f51;
	add.f32 	%f53, %f45, %f52;
	add.f32 	%f54, %f50, %f53;
	add.f32 	%f55, %f51, %f54;
	sub.f32 	%f56, %f51, %f55;
	add.f32 	%f57, %f54, %f56;
	mov.f32 	%f58, 0f3F317200;
	mul.rn.f32 	%f59, %f31, %f58;
	mov.f32 	%f60, 0f35BFBE8E;
	mul.rn.f32 	%f61, %f31, %f60;
	add.f32 	%f62, %f59, %f55;
	sub.f32 	%f63, %f59, %f62;
	add.f32 	%f64, %f55, %f63;
	add.f32 	%f65, %f57, %f64;
	add.f32 	%f66, %f61, %f65;
	add.f32 	%f67, %f62, %f66;
	sub.f32 	%f68, %f62, %f67;
	add.f32 	%f69, %f66, %f68;
	abs.f32 	%f6, %f2;
	setp.gt.f32 	%p5, %f6, 0f77F684DF;
	mul.f32 	%f70, %f2, 0f39000000;
	selp.f32 	%f71, %f70, %f2, %p5;
	mul.rn.f32 	%f72, %f71, %f67;
	neg.f32 	%f73, %f72;
	fma.rn.f32 	%f74, %f71, %f67, %f73;
	fma.rn.f32 	%f75, %f71, %f69, %f74;
	mov.f32 	%f76, 0f00000000;
	fma.rn.f32 	%f77, %f76, %f67, %f75;
	add.rn.f32 	%f78, %f72, %f77;
	neg.f32 	%f79, %f78;
	add.rn.f32 	%f80, %f72, %f79;
	add.rn.f32 	%f81, %f80, %f77;
	mov.b32 	%r10, %f78;
	setp.eq.s32 	%p6, %r10, 1118925336;
	add.s32 	%r11, %r10, -1;
	mov.b32 	%f82, %r11;
	add.f32 	%f83, %f81, 0f37000000;
	selp.f32 	%f7, %f83, %f81, %p6;
	selp.f32 	%f84, %f82, %f78, %p6;
	mov.f32 	%f85, 0f3FB8AA3B;
	mul.rn.f32 	%f86, %f84, %f85;
	cvt.rzi.f32.f32 	%f87, %f86;
	abs.f32 	%f88, %f87;
	setp.gt.f32 	%p7, %f88, 0f42FC0000;
	mov.b32 	%r12, %f87;
	and.b32  	%r13, %r12, -2147483648;
	or.b32  	%r14, %r13, 1123811328;
	mov.b32 	%f89, %r14;
	selp.f32 	%f90, %f89, %f87, %p7;
	mov.f32 	%f91, 0fBF317218;
	fma.rn.f32 	%f92, %f90, %f91, %f84;
	mov.f32 	%f93, 0f3102E308;
	fma.rn.f32 	%f94, %f90, %f93, %f92;
	mul.f32 	%f95, %f94, 0f3FB8AA3B;
	add.f32 	%f96, %f90, 0f4B40007F;
	mov.b32 	%r15, %f96;
	shl.b32 	%r16, %r15, 23;
	mov.b32 	%f97, %r16;
	ex2.approx.ftz.f32 	%f98, %f95;
	mul.f32 	%f8, %f98, %f97;
	setp.eq.f32 	%p8, %f8, 0f7F800000;
	mov.f32 	%f110, 0f7F800000;
	@%p8 bra 	$L__BB0_3;

	fma.rn.f32 	%f110, %f8, %f7, %f8;

$L__BB0_3:
	setp.lt.f32 	%p9, %f4, 0f00000000;
	setp.eq.f32 	%p10, %f3, 0f3F800000;
	and.pred  	%p1, %p9, %p10;
	setp.eq.f32 	%p11, %f4, 0f00000000;
	@%p11 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_4;

$L__BB0_7:
	add.f32 	%f102, %f4, %f4;
	mov.b32 	%r19, %f102;
	selp.b32 	%r20, %r19, 0, %p10;
	or.b32  	%r21, %r20, 2139095040;
	setp.lt.f32 	%p15, %f2, 0f00000000;
	selp.b32 	%r22, %r21, %r20, %p15;
	mov.b32 	%f112, %r22;
	bra.uni 	$L__BB0_8;

$L__BB0_4:
	mov.b32 	%r17, %f110;
	xor.b32  	%r18, %r17, -2147483648;
	mov.b32 	%f99, %r18;
	selp.f32 	%f112, %f99, %f110, %p1;
	setp.geu.f32 	%p12, %f4, 0f00000000;
	@%p12 bra 	$L__BB0_8;

	cvt.rzi.f32.f32 	%f100, %f2;
	setp.eq.f32 	%p13, %f100, %f2;
	@%p13 bra 	$L__BB0_8;

	mov.f32 	%f112, 0f7FFFFFFF;

$L__BB0_8:
	add.f32 	%f103, %f5, %f6;
	mov.b32 	%r23, %f103;
	setp.lt.s32 	%p16, %r23, 2139095040;
	@%p16 bra 	$L__BB0_15;

	setp.gtu.f32 	%p17, %f5, 0f7F800000;
	setp.gtu.f32 	%p18, %f6, 0f7F800000;
	or.pred  	%p19, %p17, %p18;
	@%p19 bra 	$L__BB0_14;
	bra.uni 	$L__BB0_10;

$L__BB0_14:
	add.f32 	%f112, %f4, %f2;
	bra.uni 	$L__BB0_15;

$L__BB0_10:
	setp.eq.f32 	%p20, %f6, 0f7F800000;
	@%p20 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_11;

$L__BB0_13:
	setp.gt.f32 	%p23, %f5, 0f3F800000;
	selp.b32 	%r27, 2139095040, 0, %p23;
	xor.b32  	%r28, %r27, 2139095040;
	setp.lt.f32 	%p24, %f2, 0f00000000;
	selp.b32 	%r29, %r28, %r27, %p24;
	mov.b32 	%f104, %r29;
	setp.eq.f32 	%p25, %f4, 0fBF800000;
	selp.f32 	%f112, 0f3F800000, %f104, %p25;
	bra.uni 	$L__BB0_15;

$L__BB0_11:
	setp.neu.f32 	%p21, %f5, 0f7F800000;
	@%p21 bra 	$L__BB0_15;

	setp.ge.f32 	%p22, %f2, 0f00000000;
	selp.b32 	%r24, 2139095040, 0, %p22;
	or.b32  	%r25, %r24, -2147483648;
	selp.b32 	%r26, %r25, %r24, %p1;
	mov.b32 	%f112, %r26;

$L__BB0_15:
	ld.const.f32 	%f109, [K];
	setp.eq.f32 	%p26, %f2, 0f00000000;
	setp.eq.f32 	%p27, %f4, 0f3F800000;
	or.pred  	%p28, %p27, %p26;
	selp.f32 	%f105, 0f3F800000, %f112, %p28;
	mul.f32 	%f106, %f109, %f105;
	mul.f32 	%f107, %f4, %f4;
	div.rn.f32 	%f108, %f106, %f107;
	shl.b64 	%rd8, %rd2, 2;
	add.s64 	%rd9, %rd1, %rd8;
	st.global.f32 	[%rd9], %f108;

$L__BB0_16:
	ret;

}

